\section{Background and Related Work} 

In modern commercial buildings, metadata
is often expressed as short text strings with several concatenated abbreviations
in a point name. Table~\ref{table:ex} lists a few point names of sensors from
three different building management systems
(Trane\footnote{\url{http://www.trane.com/}},
Siemens\footnote{\url{http://www.siemens.com/}} and Barrington
Controls\footnote{The company is no longer in business.}). For example, the
point name \texttt{SODA1R300\_\_ART} is constructed as a concatenation of the
building name (\texttt{SOD}), the air handler unit identifier (\texttt{A1}), the
room number (\texttt{R300}) and the sensor type (\texttt{ART}, area room
temperature). As the name indicates, this point measures the temperature in a
particular room; and it also indicates the control unit that can affect the
temperature in this room. Clearly, different naming conventions are used in
these buildings. For example, the notion of {\em room temperature} is encoded
with a different abbreviation in each of the three buildings: \texttt{Temp},
\texttt{RMT} and \texttt{ART}. Such variations across different buildings impose
great difficulty in quickly deploying automated analytic solutions.

% \subsection{Metadata Heterogeneity}
\begin{table}[h]
\centering
\begin{tabular}{c|ll}
\cline{1-2}
Building & Point Name & \\
\cline{1-2}
\multirow{2}{*}{\texttt{A}}  & \texttt{Zone Temp 2 RMI204} &  \\
					& \texttt{spaceTemperature 1st Floor Area1} &  \\ \cline{1-2}
\multirow{2}{*}{\texttt{B}} & \texttt{SDH\_SF1\_R282\_RMT} &  \\
                     & \texttt{SDH\_S1-01\_ROOM\_TEMP} &  \\ \cline{1-2}
\multirow{2}{*}{\texttt{C}}  & \texttt{SODA1R300\_\_ART} &  \\
					  & \texttt{SODA1R410B\_ART} &  \\ \cline{1-2}
\end{tabular}
\caption{Example point names for temperature sensors from three different buildings.}
\label{table:ex}
\end{table}


To the best of our knowledge, we are the first to develop transfer learning
based solutions to address the problem of sensor type classification across
buildings.

Researchers have tried to systematically address the problem of point name
normalization. Dawson-Haggerty et al.~\cite{boss} and Krioukov et al.~\cite{bas}
introduce a Building Operating System Service stack, whereby the underlying
building sensor stock is presented to applications through a driver-based model
\ and an application stack provides a fuzzy-query based interface to the
namespace exposed through the driver interface. Although this architecture has
some useful properties for easing generalizability across buildings, the driver
registration process is still performed manually. Bhattarcharya et
al.~\cite{arka} exploit a programming language based solution, where they derive
a set of regular expressions from a handful of labeled examples to normalize the
point name of sensors. This approach assumes a consistent format for all point
names across buildings, which might not be true in practice (as shown in Table
\ref{table:ex}). Schumann et al.~\cite{ibm} develop a probabilistic framework to
classify sensor types based on the similarity of a raw point name to the entries
in a manually constructed dictionary. However, the performance of this method is
limited by the coverage and diversity of entries listed in the dictionary, and
the dictionary size becomes intractable when there exist a lot of variations of
the same type, or conflicting definitions of a dictionary entry in different
buildings. Hong et al~\cite{cikm} formulate an active learning based approach to
iteratively acquire human labels for informative examples and propagate the acquired labels among points. However, all aforementioned work depends on manual
annotations, and thus none of them address the scalability issue of metadata
normalization across buildings nor leverage the knowledge from already labeled
buildings.

Applying transfer learning to cross building sensor type classification saves
extra effort in manual annotation by exploiting the labels in the already well
labeled buildings. There are several categories of transfer learning, e.g.,
inductive, transductive, and multi-task transfer learning as comprehensively
surveyed in~\cite{transfer1}. % We only briefly summarizes the differences.
Inductive transfer learning~\cite{transfer2} assumes the set of class labels in
the target domain is different from those in the source domain, and aims at
achieving high classification performance in the target domain by transferring
knowledge from the source domain. Multi-task transfer learning \cite{multitask}
has a similar setting, but tries to learn from the target and source domains
simultaneously. Transductive transfer learning~\cite{transfer3} assumes the
source and target domains have the same set of labels, but different marginal
distribution of features (i.e., $p(x)$) or conditional distribution of labels
(i.e., $p(y|x)$). This breaks the basic identical and independent assumption in
classical supervised learning models and makes them inept. Typical solutions in
transductive transfer learning reweight the source domain trained classifiers'
predictions in the target domain, e.g., instance-based local
weighting~\cite{weight1,weight2,weight3}. But these solutions usually assume
that only the marginal distribution of features differ in the source and target
domain. Ensemble methods are therefore explored to assign different weights to a
set of classifiers to accommodate the varying conditional probabilities of
labels in the target domain \cite{ensem1,ensem2}. Our problem setting falls into
this category: we assume we have well-labeled instances from one source building, but do
not have any labeled instances in the target building. We exploit different
properties of a sensor point to perform the transfer learning: sensor's  data is
utilized to estimate a diverse set of classifiers to transfer knowledge from the
source building to target building; sensor names in the target building are used
to compute the ensemble weight of classifiers during knowledge transfer. To the
best of our knowledge, we are the first to explore multiple types of features in
transfer learning.

% Beyond ensemble-based transfer learning, we explore two types of attributes of a sensor: the features extracted from sensor reading streams are used to estimate a set of different classifiers in source domain; sensor name strings are used to generate clusters in the target domain, and those clusters are then used to assign weights to the source domain trained classifiers when integrating their predictions.
% Such knowledge transfer is possible when the training domain and the test domain have the same set of class labels.


%Timeseries Representation~\cite{sax,shapelet1,shapelet2}

% The use of transfer learning is motivated by the fact that people often have one or a few buildings labeled of which they want to take advantage to aid the labeling of a new building.
% Transfer learning is a useful technique in the building space because the effort it takes to label sensors in a single building is high.
% We want to leverage the knowledge gained in one building to quickly label another with minimal effort.
% One category of transfer learning uses well-labeled data from one domain\footnote{A ``domain'' particularly refers to a data set in this paper.}
% to classify examples in a new, related domain.
% Classical supervised learning techniques are not useful for transferring knowledge across domains in this setting because
% The reason that traditional supervised learning techniques is not successful in transferring knowledge across domains in our case is because
% it requires the training and testing examples to be sampled i.i.d. from the same distribution. This basic requirement does not hold here.
