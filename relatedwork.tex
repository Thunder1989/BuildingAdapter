\section{Related Work}
To the best of our knowledge, we are the first to approach the problem of sensor type classification leveraging knowledge across buildings.

Researchers have tried to address the non-uniformity of sensor namings asystematically.
Dawson-Haggerty et al.~\cite{boss} and Krioukov et al.~\cite{bas}
introduce a Building Operating System Service stack, whereby
the underlying building sensor stock is presented to applications through a driver-based model \
and an application stack provides a fuzzy-query based interface to the namespace exposed 
through the driver interface.
Although this architecture has useful properties for easing generalizability across
buildings, the driver registration process is still done manually. 
Bhattarcharya et al~\cite{arka} exploit a programming language based solution, 
where they derive a set of regular expressions from a handful of labeled examples 
to normalize the point name of sensors. 
This approach assumes a consistent format for all point names, which is not the case in practice, as shown in Table~\ref{table:ex}. 
Schumann et al~\cite{ibm} develop a probabilistic framework to classify sensor types 
based on the similarity of a raw point name to the entries in a manually constructed dictionary. 
However, the performance of this method is limited by the coverage and diversity of entries listed in the dictionary, and the dictionary size becomes intractable when there exist a lot of variations of the same type, or conflicting definitions of a dictionary entry in different buildings.
Hong et al~\cite{cikm} formulate an active learning based approach to iteratively 
acquire human labels for the building and propogate pseudo labels among points.
However, none of these prior work addresses the scalability issue of metadata 
normalization nor leverages the knowledge from already labeled buildings.

There are several categories of tranfer learning as comprehensively surveyed in~\cite{transfer1}. We only briefly summarizes the differences. Inductive transfer learning~\cite{transfer2} assumes the set of class labels in the target domain is different from the source domain, no matter the feature spaces are the same or not. Multi-task learning~\cite{multitask} has a similar setting, but inductive transfer learning only aims at achieving high performance in the target task by transferring knowledge from the source task while multitask learning tries to learn the target and source task simultaneously. Transductive transfer learning~\cite{transfer3} assumes the source and the target have the same set of labels no matter the feature spaces are the same or not; our problem setting falls into this category. There has also been active research on how to construct an ensemble of classifiers and some assign local weights locally~\cite{ensem1,ensem2}, however, these local weights are decided only based on the training data.
Instance-based local weighting~\cite{weight1,weight2,weight3} has also been well studied, but these work assumed that the training and testing distributions differ only in $p(x)$ but not in $p(y|x)$.

%Timeseries Representation~\cite{sax,shapelet1,shapelet2}