\begin{figure*}[ht!]
\centering
  \begin{subfigure}{0.32\textwidth}
                \centering
    \includegraphics[width=\textwidth]{./fig/co2.eps}
                \caption{$CO_{2}$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
                \centering
    \includegraphics[width=\textwidth]{./fig/rh.eps}
                \caption{Humidity}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
                \centering
    \includegraphics[width=\textwidth]{./fig/rmt.eps}
                \caption{Room Temperature}
  \end{subfigure}
  %row2
  \begin{subfigure}{0.32\textwidth}
                \centering
    \includegraphics[width=\textwidth]{./fig/stpt.eps}
                \caption{Room Temperature Set Point}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
                \centering
    \includegraphics[width=\textwidth]{./fig/vav.eps}
                \caption{VAV Air Volume}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
                \centering
    \includegraphics[width=\textwidth]{./fig/cwt.eps}
                \caption{Chilled Water Supply Temperatere}
  \end{subfigure}
\caption{Different type of sensor occupies different amplitude bins in the time domain with different short term dynamics.}
\label{fig:example}
\end{figure*}

\section{Methodology}
We first describe how we design the feature extraction and explain why the feature set works. Then we discuss
the classification technique we adopt as well as detail the training and testing process. In the end, we articulate
a solution to identifying potential misclassification when no ground truth labels of sensor type are available.

\subsection{Feature Extraction}
Raw sensor time series\footnote{In this paper, we use the term ``trace'', ``readings'' and ``time series'' \textit{interchangably}.} usually contain millions of readings which are too general to be informative for classification
tasks. We need to distill the information embedded in the sensor readings in the time domain. A signal in the time domain
trends the amplitude of a sensor reading and intuitively, different types of sensor would in general occupy distinct
amplitude bins as demonstrated in Figure~\ref{fig:example}. As to characterize the amplitude distribution of a signal in the time
domain, we can use the percentiles of readings, such as 25th, 50th, 75th and so forth. Considering that there also can
exist outlieres in sensor readings, we pick the 50th (also known as the median) for use as a discriminator, which
is more robust to outliers. However, on another side, sensor readings are subjective to the dynamics in the proximate
environment therefore sensors of different types can collide in a same amplitude bin. For example, during a rainy season, the humidity in an
office can reach the range of 60-70 which is the same as typical temeprature sensor readings. Therefore, simply relying
on some percentile of reading amplitude might not be able to effetively differentiate sensor types. Figure~\ref{fig:same_bin} show such an
example. To capture these ``changes'' as features, we also need to include the variance of the signals when
formulating a feature vector for the time series.

\begin{figure}[ht!]
\centering
  \begin{subfigure}{0.22\textwidth}
                \centering
    \includegraphics[width=\textwidth]{./fig/rmt_ex.eps}
                \caption{Room Temperature}
  \end{subfigure}
  \begin{subfigure}{0.22\textwidth}
                \centering
    \includegraphics[width=\textwidth]{./fig/rh_ex.eps}
                \caption{Humidity}
  \end{subfigure}
\caption{An example of two differernt type of sensors occupying the same amplitude bin.}
\label{fig:same_bin}
\end{figure}

When we extract features from a raw sensor reading, the original trace can span over hour, days or even weeks, and
the trend can vary significantly even from hour to hour. On one side, extracting certain features such as percentiles and
variances over the entire sensor reading might miss too many short term dynamics thus making the features less discriminant, compared to doing so in shorter
windowed time slices. On the other side, computing features over windowed slices can produce too many elements for a
feature vector and also, having too many unnecessary feature variables might degrade the performance of classifier.
To better and succintly summarize the dynamics of sensor traces, we apply feature extraction to every time window of fixed length on the original trace and compute the statistics of the accumulated features from windowed slices as the final features. 

As a summary, the feature extraction procedure goes as follows. First, each single sensor signal is segmented into N
non-overlapping 45-minute long windows (we will discuss the decision of window length in later section). Second, within
each time window, we compute the median and variance of the signal, producing a vector of medians and a vector of
variances after the window slides over the entire traces: 
\begin{displaymath}
\begin{split}
MED = \{median^{1}, median^{2}, ..., median^{N}\}\\
VAR = \{variance^{1}, variance^{2}, ..., variance^{N}\}
\end{split}
\end{displaymath}
Where N is the number of time windows. The vector $MED$ and $VAR$ reflect short term changes but not all the intermidiate values are essentially useful for classification. So as a statistical summary of the two vectors, in a last step, for each vector we compute the minimun, maximun, median and variance, resulting in a feature vector of eight variables:
\begin{displaymath}
\begin{split}
F = \{min(MED), max(MED), median(MED), var(MED),\\
 min(VAR), max(VAR), median(VAR), var(VAR)\}
\end{split}
\end{displaymath}
And $F$ is the feature vector for each sensor trace used in our classification process.

\subsection{Classification}
After transforming all sensor time series into feature vectors, we leverage an ensemble classifier to achieve 
the type classification task. In general, ensemble learning methods obtain better predictive performance than
 any of the constituent learning methods~\cite{ensem1,ensem2}, if two assumptions hold~\cite{ensem3}: 1) the 
 probability of a correct classification by each individual classsifier is greater than 0.5 and 2) the errors
  in predictions of the base-level classifiers are independent. Random forests~\cite{RF} have been widely used 
  and outperform a single tree classifier. They are also faster~\cite{cvpr} in training and testing compared to traditional classifiers such as SVM. The notion of randomized trees was first introduced in~\cite{RT} and further well developed in~\cite{RF}.

Random forests grow a bunch of classification trees. To classify a new coming unlabeled object as a feature vector, 
feed the vector down each of the trees in the forest. Each tree gives a classification, in other words, the 
tree ``votes" for that class. The forest chooses the class having the most votes over all the trees in the forest. 
As a quick overview of how each tree is grown in the forest, the process goes as follows:
\begin{enumerate}
\item Sample N instances at random with replacement\footnote{An element may appear multiple times in the sample set.}, from the original data set. These samples will be the training set for growing this particular tree.
\item Specify M feature variables at random out of the total feature vector when growing each node of a tree. And the best split (measured by the information gain) on these M is used to split the node. The value of M is constant during the forest growing.
\item Each tree is grown to the largest extent possible without pruning.
\end{enumerate}
The randomness of this ensemble learning method originates in the first two steps and specifically, we set N equal the number of instances in the original training set, M equal the square root of the
number of original features, and the number of the trees in the forest be 50. Usually these parameters are optimized
through cross-validation and we refer interested readers to~\cite{RF} for further deduction and proof of random forest.

All the instances in our data set are labeled with ground truth class and to train a random forest, we split the original set into two subsets, one for building 
a forest and one for testing the accuracy of the classifier. After the forest is built from training set, we learn the posterior probabilities of each class $c$ 
at each leaf node $l$ for each tree $t$: suppose that $T$ is the set of all trees, $C$ is the set of all classes and $L$ is the set of all leaves for a given 
tree $t$. In the training stage the posterior probabilities $P_{t,l}(Y(i) = c))$ for each class $c\in C$ at each leaf $l\in L$, are learned for each tree $t\in T$. 
These probabilities are calculated as the ratio of the number of instance $i$ of class $c$ that reach $l$ to the total number of intances that reach $l$. $Y(i)$ 
is the class label for instance $i$. To classify an instance in the testing set, the feature vector of an instance is passed down each tree until reaching a leaf 
node, which gives a probability distribution. All the posterior probabilities accumulated from each tree are then averaged and the $argmax$ is taken as the class 
of the instance. 

% \begin{algorithm}[h!]
%  \SetAlgoLined
%  Give signal $X(t)$:\\
%   \While{the \# of maxima in $X(t)$ >3}{
%   (1) identify all the local extrema in $X(t)$\;
%   (2) perform a cubic spline interpolation of maxima to get the upper envelope\;
%   (3) repeat (2) on minima to get the lower envelope\;
%   (4) $h(t) = X(t) - mean((2),(3))$ \;
%   (5) repeat (2)-(4) until $h(t)$ is an IMF\;
%   (6) $X(t) = X(t) - h(t)$, and return the IMF\;
%   }
%  \caption{Empirical Mode Decomposition}
%  \label{alg:emd}
% \end{algorithm}

\subsection{Quantify the Uncertainty of Classification}
It is an easy job to identify misclassification when we have groud truth labels, but in many of the cases motivating
our work the ground truth labels are not available, therefore identifying potential misclassification
would suffer from the absence of ground truth. However, if we can quantify the uncertainty of classification, that might help identify potential misclassified 
instances. To quantify the uncertainy of classification in our job, we leverage the posterior probabilities learned in random forest.

With the learned posterior probabilities for each class at each leaf in each tree, given a new coming instance, we can compute the averaged probabilities for each class as:
\begin{displaymath}
    \bar P(Y(i)=c) = \frac{\sum_{t} P_{t,l}(Y(i)=c)}{|T^{'}|}, t\in T^{'}
\end{displaymath}
Where $T^{'}$ is the collection of trees in the forest where $P_{t,l}(Y(i)=c)\neq 0$, and $|\cdot|$ denotes the cardinality of a set. Given these averaged 
probabilities for each class, the random forest will produce a vector of class probability for each new coming instance:
\begin{displaymath}
\textbf{Pr} = \{\bar P(Y(i)=c)\}, c\in C
\end{displaymath}
Suppose we get a probablity vector $Pr_{1} = \{0.9, 0, 0, 0, 0, 0.1\}$ for intance $i_{1}$ and another vector $Pr_{2} = \{0.3, 0.25, 0.1, 0, 0.15, 0.2\}$ for intance $i_{2}$. Both $i_{1}$ and $i_{2}$ will be assigned to the class according to the class probability distribution, but the assignment of $i_{2}$ is less certain compared to that of $i_{1}$ because its predicted class probability has a less ``concentrated'' distribution. As to measure this degree of ``uncertainty'' in classification of one instance, we compute the entropy of its class probability. Based on this entropy metric, we can rank the classfications and pick out the instances for further manual inspection whose entropy is larger than some threshold, and the inspection can help eliminate misclassifications.
