\section{The Building Adapter}
Based on the insight that sensor reading data and sensor names characterize different properties of a sensor stream, we develop a transfer learning based approach to exploit features extracted from both sensor readings and point names to perform the knowledge transfer. Specifically, we construct classifiers\footnote{We use the term ``classifier'', ``model'' and ``learner'' {\it interchangeably}.} based on data features because they are more likely to be consistent across buildings.
%why ensemble
However, a single supervised learner might not perform well on all examples in the new building due to the inductive bias inherent in classifier training. Hence, we employ an ensemble of classifiers, where each classifier captures a different ``perspective'' in predicting the sensor type.
%then we need local weight
When applied to a new building, since different classifiers might be effective in predicting different instances, we appeal to the instance-specific local weighting method proposed in~\cite{lwe} to weight those different classifiers while ensemble.
In our solution, the weighting is derived from the consistency between a model's predictions and the instance's local clustering structure, which is estimated by the sensor names in the target building.
% In particular, we first estimate a set of statistical classification models based on features extracted from the sensor data from one building, and transfer the learned models to label sensors in another building.
% We weight the models for each instance in the target building by measuring the consistency between a model's predictions and the instance's local structure that is approximated by clustering: classifiers with higher consistency would be assigned a larger weight.
% while name features are more suitable for discovering unsupervised neighborhood with clustering.
In the rest of the section, we first elaborate how we construct the two different types of features and then concentrate on the proposed transfer learning method.


\subsection{Feature Representation}\label{feature}
Our solution exploits two common attributes of the sensor points in a building - the actual sensor reading data and the text string-based point names, both of which play an important role for differentiating sensor types.
In this section, we describe the construction of two different sets of features, i.e., data features and name features.

\begin{figure*}[ht!]
\centering
  \begin{subfigure}{0.32\textwidth}
                \centering
    \includegraphics[width=\textwidth]{./fig/co2.eps}
                \caption{$CO_{2}$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
                \centering
    \includegraphics[width=\textwidth]{./fig/rh.eps}
                \caption{Humidity}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
                \centering
    \includegraphics[width=\textwidth]{./fig/rmt.eps}
                \caption{Room Temperature}
  \end{subfigure}
  %row2
  \begin{subfigure}{0.32\textwidth}
                \centering
    \includegraphics[width=\textwidth]{./fig/stpt.eps}
                \caption{Room Temperature Set Point}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
                \centering
    \includegraphics[width=\textwidth]{./fig/vav.eps}
                \caption{VAV Air Volume}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
                \centering
    \includegraphics[width=\textwidth]{./fig/cwt.eps}
                \caption{Chilled Water Supply Temperature}
  \end{subfigure}
\caption{Different types of sensor generally have different amplitudes that can be separated and binned to characterize the data.}
\label{fig:example}
\end{figure*}

\subsubsection{Data Features}
A signal\footnote{In this paper, we use the term ``signal'', ``trace'' and ``time series'' \textit{interchangeably}.} in the time domain is a trend of sensor reading.
Different types of sensors generally have different amplitudes that can be separated and binned,
as demonstrated in Figure~\ref{fig:example}.
Similar to piecewise aggregated approximation (PAA~\cite{paa}) -- where the mean is calculated in a fixed-length window --
we compress the signal by computing a set of summary statistics over fixed-length windows.
Table~\ref{table:fd} shows a summary of the statistics we calculate as data features.


\begin{table}[h]
\centering
\begin{tabular}{r|l|l}
\hline
Category                   & Statistical Function & \multicolumn{1}{l}{Acronym} \\ \hline\hline
\multirow{2}{*}{Extrema}   & Minimum                 & min                          \\ \cline{3-3}
                           & Maximum                 & max                          \\ \hline
\multirow{2}{*}{Average}   & Median                  & emd                          \\ \cline{3-3}
                           & Root Mean Square        & rms                          \\ \hline
\multirow{2}{*}{Quartiles} & 1st and 3rd Quartiles   & 1q, 3q                       \\ \cline{3-3}
                           & Inter-quartile range    & iqr                          \\ \hline
\multirow{3}{*}{Moments}   & Variance                & var                          \\ \cline{3-3}
                           & Skewness                & skew                         \\ \cline{3-3}
                           & Kurtosis                & kurt                         \\ \hline
Shape                      & Linear Regression Slope & slope                        \\ \hline
\end{tabular}
\caption{Statistical features extracted in window level for each time series data.}
\label{table:fd}
\end{table}

Our feature extraction process consists of three steps.
First, each sensor trace is segmented into $N$ hour-long windows with 50\% overlap between consecutive windows. Second, for each time window, we compute the statistics shown above.
For example, the vector for $MIN$ is computed as follows:
$MIN = \{min^{1}, min^{2}, ..., min^{N}\}$, where $N$ is the number of time windows. We compute a similar vector for each statistic show in the table.
Third, we compute a statistical summary of these vectors. For each vector we compute the minimum, maximum, median and variance, resulting in a feature
vector containing 44 variables:
\begin{displaymath}
\begin{split}
F = \{min(MIN), max(MIN), \\
median(MIN), var(MIN),\\
...\\
min(SLOPE), max(SLOPE), \\
median(SLOPE), var(SLOPE)\}
\end{split}
\end{displaymath}
$F$ is the data feature vector for each sensor stream used in our study.

\subsubsection{Name Features}
Sensor point names are short text strings with several concatenated abbreviations, as shown in Table~\ref{table:ex}.
To extract features from a point name, we convert all alphabetical characters to lower cases and trim out numerical characters.
For example, \texttt{Zone Temp 2 RMI204} becomes \texttt{\{zone, temp, rmi\}}.
We use $k$-mers \cite{leslie2004mismatch} to capture variations in type abbreviations, e.g., ``tmp'' and ``temp'' for temperature.
The term $k$-mer refers to all the possible substrings of length $k$ in a given string. This feature is often used in protein and gene sequence analysis and
it helps measure sequence similarity without requiring alignment.

We limit the k-mers generation within a word boundary.
In general, a smaller $k$ will increase the overlapping between the generated k-mers, making points less differentiable.
Therefore, we compute all k-mers of length 3 and 4 for each point name.
For example, \texttt{\{zone, temp, rmi\}} yields a set of k-mers \texttt{\{zon, one, tem, emp, rmi\}} when $k$=3.
A dictionary of k-mers is constructed with all the k-mers generated from each point name.
Each point name is converted into a feature vector based on the frequency of k-mers in it.
For example, a set of k-mers \texttt{\{zon, tem, emp, zon\}} will be transformed to a vector
\texttt{(2,0,1,1,0)} with the dictionary \texttt{\{zon, one, tem, emp, rmi\}}, meaning \texttt{zon} occurs twice, \texttt{one} does
not appear, and so forth.
This feature representation of point names will be used for clustering on the new building.


\subsection{Locally Weighted Ensembles for Knowledge Transfer}
To effectively encapsulate and transfer knowledge from one labeled building to another,
we construct a set of different classifiers, including support vector machines \cite{suykens1999least}, logistic regression \cite{hosmer2004applied} and random forest \cite{liaw2002classification},
using the same set of data features from the source building.
Each classifier presents a different ``perspective'' for recognizing sensor types in the source building, due to the inductive bias of the underlying model.  We refer to these classifiers as {\it base} models in this paper, which will be combined for type classification in the target building.

%We weight the classifiers in the target building by measuring the consistency between a classifier's predictions and the instance's local structure that is approximated by clustering: classifiers with higher consistency would be assigned a larger weight. Based on our previous observations, classifiers constructed based on data features are more likely to transfer across buildings, while name features are more suitable for discovering unsupervised neighborhood with clustering. We will next elaborate the intuition and details behind employing instance-based local weighting and choosing clustering for local structure approximation. Finally, we describe how we identify the cluster structure with a non-parametric Bayesian method.
% We will discuss the choice of data features for classifier training in the evaluation.

The performance of any particular model can vary from building to building, and different models can be effective at different regions or structures in a target building: no single model can perform well on all examples.
%Different models can be effective at different regions or structures in a target building, and
Therefore, to combine the knowledge in base models,
% rather than only using any particular model, to more effectively transfer the knowledge to the new building.
% A natural choice is model averaging that additively combines the predictions of multiple base learners.
% However, the existing model averaging methods for traditional supervised learning usually assign global weights to models, which are either uniform (e.g., in Bagging~\cite{bagging}) or proportional to the training accuracy (e.g., in Boosting~\cite{boosting}), or simply relying on a specific model (e.g., single model classification).
% Such global weighting schemes may not perform well in transfer learning because different testing examples may favor predictions from different base models.
% For example, when the base models carry conflicting concepts at a testing example, the optimal choice would be a model that better represents the distribution underlying the example.
we employ the method from~\cite{lwe} to locally weigh each classifier and combine the predictions from different base classifiers in the target building.
The weight is computed per model per example based on the consistency between the base models' predictions and the local clustering structure of the target example.
% The consistency is measured by comparing neighborhood graphs constructed by the name features. The construction of this neighborhood graph will be explained later.
Intuitively, the estimated local weights will favor models whose predictions are consistent with the estimated local structure of the target example in the target building.

Formally, let $x$ be the data feature vector of an instance in the target building and $y$ be its predicted class label. Given a set of $k$ base models $M_1, \dots, M_k$ and the new testing set $D_T$ encoded with the data features, the general Bayesian model averaging rule estimates the posterior distribution of $y$ in $x$ as,
\begin{equation}\label{eq_lwe}
p(y|x)=\sum_{i=1}^k p(y|x,D_T,M_i) p(M_i|D_T)
\end{equation}
where $p(y|x,D_T,M_i) = p(y|x,M_i)$, because $x \in D_T$ and $p(y|x,M_i)$ is the label for $x$ predicted by $M_i$ and $p(M_i|D_T)$ is the probability of choosing $M_i$ given the testing set $D_T$.
Since $x \in D_T$, $p(M_i|D_T)$ is equal to $p(M_i|x)$, which is the locally adjusted weight for $M_i$, and Eq.~\ref{eq_lwe} becomes,
\begin{equation}\label{eq_sum}
p(y|x)=\sum_{i=1}^k w_{x}^{M_i} p(y|x, M_i)
\end{equation}
where $w_{x}^{M_i} = p(M_i|x)$.  A model $M_i$ is expected to have a higher weight for $x$ if $M_i$'s predicted local structure for $x$ is closer to the estimated neighborhood.
We will next explain how the weight is calculated per example.

\subsection{Graph-based Weight Estimation}\label{sec:gwe}
% If $p(y|x)$ is known, we can estimate the weights $w_{x}^{M_i}$ for each model by minimizing the square error between its prediction and the ground truth.
% However, in practice the truth value of $p(y|x)$ for a new building is not available a priori .
Ideally a larger weight should be assigned to the model whose predicted neighborhood structure of a testing instance in the target building is most consistent with its true neighborhood. To locally weight the models in such a manner, we appeal to
% We quickly summarize the graph-based weight estimation technique presented in~\cite{lwe} and describe two adjustments we make to fit our problem.
%Assuming that $p(y|x)$ does not change much in a dense area where $p(x)$ is high~\cite{cluster},
% which means the decision boundary probably exists in areas with smaller $p(x)$.
the technique proposed in~\cite{lwe}: it performs clustering on the new testing set in the target domain and assumes if the clustering boundary for the region where $x$ falls, agrees with the decision boundary of $M_i$,
% we assume that $p(y|x,M_i)$ is similar to the true $p(y|x)$ around $x$, which means
a larger weight should be assigned to $M_i$ at point $x$.
In other words, if the predictions made by $M_i$ on the area surrounding $x$ have greater consistency with the clustering results, $M_i$ will be assigned
a larger weight at $x$.

Based on these assumptions, the weight can be estimated with a graph-based algorithm.
To compute the $w_{x}^{M_i}$ for $M_i$ at point $x$, we construct two neighborhood graphs:
$G_M = (V, E_M)$ and $G_C = (V, E_C)$, by classification and clustering results respectively,
where each vertex is an instance in the target domain $D_T$. In the graph constructed by model $M_i$, i.e., $G_{M_i}$, an edge exists between two vertices (denoting the two examples are ``neighbors'') if and only if $M_i$ assigns the same label for these two examples. Likewise, in $G_C$, an edge exists between two vertices if and only if these two examples reside in the same cluster.
If the neighbors of $x$ on both graphs significantly overlap, then $M_i$ will be assigned a larger weight in ensemble.
So the weight $w_{x}^{M_i}$ for $M_i$ at $x$ is proportional to the similarity of the two graphs:
\begin{equation}\label{eq_sim}
w_{x}^{M_i} \propto s(G_M, G_C|x) = \frac {|V_M \cap V_C|} {|V_M \cup V_C|}
\end{equation}
where $V_M$ ($V_C$) is the set of neighbors of $x$ on graph $G_M$ ($G_C$), $|X|$ is the cardinality of set $X$, and $s(G_M, G_C|x)$ denotes the similarity between two graphs.
Figure~\ref{fig:graph} illustrates an example of neighborhood graphs for an example $x$ (in grey circle): model 1 has a similarity of 0.75 with the cluster graph
 while model 2 has a similarity of 0.5 with the cluster graph.

\begin{figure}[h]
\centering
    \includegraphics[width=0.4\textwidth]{./fig/lwe_graph}
\caption{An example of local neighborhood graphs of $x$ (in grey circle).}
\label{fig:graph}
\end{figure}

%With Eq.~\ref{eq_sim} defined,
The weight for each $M_i$ is calculated by normalizing among all similarity scores:
\begin{equation}\label{eq_norm}
w_{x}^{M_i} = \frac {s(G_{M_i}, G_C|x)} {\sum_{i=1}^k s(G_{M_i}, G_C|x)}
\end{equation}
The final prediction for $x$ is simply $\hat y = argmax_y \enspace p(y|x)$ as $p(y|x)$ is defined in Eq.~\ref{eq_sum}.

\subsubsection{Distance-based Adjustment}
In some cases the similarity score defined in Eq~\ref{eq_norm} can be problematic. Consider the case shown in Figure~\ref{graph_dist}.
The example shows two models where the target example has two neighbors.  The distance between the examples are marked on the edge.
Since the original similarity definition only considers the number of neighbors, both models will be assigned with the same weight of 0.5.
However, the neighbors in model 1 are closer to the target example than those in model 2.  Therefore model 1 should be assigned a higher
weight.
To fix the issue we include the distance between examples into consideration and adjust similarity score as,
\begin{equation}\label{d_sim}
s^\ast(G_M, G_C|x) = 1 - \frac {\sum d_{V_I}/|V_I|} {\sum d_{V_U}/|V_U|}
\end{equation}
where $V_I = V_M \cap V_C$, $V_U = V_M \cup V_C$, and $\sum d_{V_I}$ is the sum of distance between $x$ to its neighbors in $V_I$ (likewise for $\sum d_{V_U}$).

\begin{figure}[h]
\centering
    \includegraphics[width=0.4\textwidth]{./fig/lwe_d_graph}
\caption{An example of local neighborhood graphs of $x$ with distance into consideration.}
\label{graph_dist}
\end{figure}

\subsubsection{Thresholding-based Adjustment}
The use of a \emph{weighted-average decision} for $x$ among base classifiers is reasonable when at least some classifiers perform well on $x$.
However, the similarity $s(G_{M_i}, G_C|x)$ for $M_i$ is expected to be small when the predictions of $M_i$ around $x$ conflict with its true local structure.
In such a case, using the decision from this classifier might be misleading.
Since $s(G_{M_i}, G_C|x)$ reflects the consistency between $M_i$'s predictions and the testing instance $x$'s local clustering structure, we use the average similarity score over all $M_i$ on $x$ as a discriminator to limit the usage of these base classifiers.
As an adjustment before the normalization of similarity scores, we check the average similarity score with,
\begin{equation}\label{ave_sim}
 \bar s_x = \frac {1}{k}\sum_{i=1}^k s(G_{M_i}, G_C|x)
\end{equation}
We continue with the ensemble of predictions only if $\bar s_x$ is larger than a threshold $\delta$, and we will discuss the choice of $\delta$ in the evaluation section.


\subsection{Non-parametric Bayesian Clustering}
\label{sec_clustering}
In our transfer learning based approach, clustering structure for instances in target building is exploited to approximate the true local structure of an testing instance.
In particular, name feature distribution $p(x)$ is exploited via its latent clustering structure.
We choose Gaussian Mixture Model (GMM) \cite{zivkovic2004improved}, a partitional clustering algorithm, to perform the clustering.

In GMM, the cluster label for every instance is treated as a latent variable, which is drawn from a multinomial distribution $p(c)$, i.e., $p(c)\propto\alpha_c$, where $\forall c, \alpha_c\ge0$ and $\sum_c\alpha_c=1$. In any given cluster $c$, the conditional data likelihood of an instance $x$ is specified by a multivariate Gaussian distribution. To reduce the number of parameters to estimate, we choose the isotropic Gaussian in our solution,
\begin{equation}
p(x|c)=(2\pi\sigma^2)^{-d/2}\exp{-\frac{(x-\mu_c)^\mt (x-\mu_c)}{2\sigma^2}}
\end{equation}
where the variance $\sigma^2$ is shared by all the clusters. $\{\alpha_c, \mu_c\}^k_{c=1}$ and $\sigma$ are considered as model parameters in GMM.

However, in GMM, we need to manually specify the number of clusters for a given input data set; and the clustering result of GMM is very sensitive to this setting. More importantly,
in our study, usually there is more than one pattern in the point names even for the same type of sensors; therefore we cannot assume a class has only one cluster. It is impossible for us to predefine the
optimal cluster sizes. To make clustering feasible on the new building, we use a non-parameter Bayesian solution: we assume the model parameters $(\alpha, \mu)$ in each cluster are also random variables, which are drawn from a Dirichlet Process prior \cite{dp}.

A Dirichlet Process $DP(G_0, \eta)$ with a base distribution $G_0$ and a scaling parameter $\eta$ is a distribution over distributions~\cite{dp}. The base distribution $G_0$ specifies the prior distribution of model parameters, e.g., mean parameter $\mu$ in each cluster, and the scaling parameter $\eta$ specifies the concentration of samples drawn from the DP, e.g., cluster proportion $p(c)$. An important property of the DP is that though the draws from a DP have countably infinite size, they are discrete with probability one, which leads to a probability distribution on partitions of the data. The number of unique draws, i.e., the number of clusters, varies with respect to the data and therefore is random, instead of being pre-specified.

As a result, with the $DP(G_{0}, \eta)$ prior, the data density in a given collection of instances can be expressed using a stick-breaking representation
\cite{sethuraman1994constructive}:
\begin{equation}\label{eq_dp_density}
p(x)=\sum_{c=1}^\infty \alpha_c \mathcal{N}(x|\mu_c,\sigma)p(\mu_c|G_0)
\end{equation}
where $\alpha={\alpha}_{c=1}^\infty\sim Stick(\eta)$ represents the proportion of clusters in the whole collection. The stick-breaking process $Stick(\eta)$ for the cluster proportion parameter $\alpha$ is defined as: $\alpha'_c\sim Beta(1, \eta), \alpha_c=\alpha'_c\prod_{i=1}^{c-1}(1-\alpha'_i)$. Since the variance $\sigma^2$ is fixed in all clusters, we use a conjugate prior for $\mu$ in $G_0$, i.e., for $\forall c, \mu_{ci}\sim \mathcal{N}(a,b)$, with the assumption that each dimension in $\mu_c$ is independently drawn from a univariate Gaussian. This will greatly simplify the later on inference procedure.

Because the data density distribution defined in Eq~\eqref{eq_dp_density} only has finite support at the points of $\{\alpha_c, \mu_c\}^k_{c=1}$, we can calculate the posterior distribution of latent cluster labels in each unlabeled instance to discover the clustering structure. Following the sampling scheme proposed in \cite{neal2000markov}, we use a Gibbs sampling method to infer the posterior of cluster membership. Detailed specifications of this sampling algorithm can be found in \cite{neal2000markov}.


{\bf Putting it all together:} Algorithm~\ref{algo} summarizes our transfer learning algorithm for the problem of sensor type classification across buildings.
We start by training a few base classifiers with the data features and labeled examples in a source building. Then we generate clusters using GMM with DP priors on the name features of examples in the target building. For each example $x$ in the target building, we measure the local similarity score for each base classifier. If the average similarity is large enough, we compute the weight for each classifier at $x$ by normalizing the similarity score. Finally we calculate the weighted sum of predictions from all base classifier and obtain the label $y$ for $x$.

\begin{algorithm}[ht]
 \caption{Transfer Learning for Sensor Type Classification}
 \label{algo}
 %\SetAlgoLined
 {\bf Input}: Data features of the source building $\mathcal{D_S}=\{x^D_1,x^D_2,\dots,x^D_n\}$ and their labels $\mathcal{Y_S}=\{y_1,y_2,\dots,y_n\}$ data features of the target building $\mathcal{D_T}=\{x^D_1,x^D_2,\dots,x^D_m\}$, and name features of the target building $\mathcal{P_T}=\{x^P_1,x^P_2,\dots,x^P_m\}$\\
 {\bf Output}: predicted labels of the examples in target building $\mathcal{Y}$\\
 Initialize: Generate clusters with $DP(G_{0}, \eta)$ on name features in $\mathcal{P_T}$\\
 Train $k$ classifiers $M_1, \dots, M_k$ based on $\mathcal{D_S}$ and $\mathcal{Y_S}$\;

\For{$x^D$ in $\mathcal{D_T}$}{
Construct neighborhood graphs $G_M$ and $G_C$ for $x^D$ as defined in Section~\ref{sec:gwe} for each $M_i$;\\
Compute the similarity score for each $M_i$ with Eq.~\ref{d_sim};\\
Check the average similarity score $\hat s_x$ over all $M_i$ with Eq.~\ref{ave_sim};\\
If $\hat s_x > \delta$, then use Eq.~\ref{eq_norm} and Eq.~\ref{eq_sum} to predict the label $y$;
}
\end{algorithm}
