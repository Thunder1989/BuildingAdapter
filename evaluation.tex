% \begin{figure*}[ht!]
% \centering
% 	\begin{subfigure}{0.48\textwidth}
%                 \centering
% 		\includegraphics[width=\textwidth]{./fig/ROC_bsln.eps}
%                 \caption{Correlating the raw signals.}
% 	\end{subfigure}
% 	\begin{subfigure}{0.48\textwidth}
%                 \centering
% 		\includegraphics[width=\textwidth]{./fig/ROC_new.eps}
%                 \caption{Correlating the re-aggregated IMFs in the ``medium'' frequency band.}
%                 \label{fig:rocA}
% 	\end{subfigure}
% \caption{The ROC curves depict the sensitivity of the raw signal and mid-frequency IMFs to the threshold value. We choose the 0.2 FPR point as the boundary threshold for each room. }
% \label{fig:roc}
% \end{figure*}

\section{Evaluation}
To demonstrate the effectiveness as well as usefulness of our methodology, we evaluate our technique in two different scenarios: a) intra building, that is, the 
training and testing data for classification is taken from the same building, and b) inter buildings, where the training and testing instances are from two 
distinct buildings. Then we analyze how the window size of segment and number of training instances affect the performance of classification. We also discuss how 
each feature variable contributes to the classification and when to use certain set of features. At last, we show a small application as a case study built based on 
the generated type information in two buildings, which would be difficult to achieve in the absence of sensor type metadata. 

\subsection{Taxonamy}
In this paper, we consider 6 types of sensors, which are $CO_{2}$, humidity, room temperature, setpoint, air flow volume, other temperature. Specifically, room 
temperature includes only sensors that measure the air temperature of rooms and other temperatures covers all other temperature measurements such as supply 
air/return air/mixed air temperature and supply/return water temperature. We also put only one general type for setpoint which includes all types of setpoints 
installed in the buildings.

\subsection{Experimental Setup}
The data we used are collected in one week from two separate buildings on two campuses. One is from the Sutardja Dai Hall at UC Berkeley where the deployed 
sensors~\cite{keti, bacnet} report to an archiver~\cite{smap} periodically from every 5 seconds to every 10 minutes. The other building is the Rice Hall at 
University of Virginia, where the sensors report to a central database~\cite{trane} from every 10 seconds to every 10 minutes. The number of each type of sensors in 
each buidlings is given in table~\ref{table:spec}.

\begin{table}[ht!]
\caption{Number of Each Sensor Type}
\centering % used for centering table
\begin{tabular}{c c c}% centered columns (4 columns)
\hline %inserts single horizontal lines
Type & SDH & Rice \\ % inserts table 
%heading
\hline\hline % inserts double horizontal line
$CO_{2}$ & 1 & 2 \\ % inserting body of the table
humidity & 2 & 4 \\
room temp. & 3 & 2 \\
setpoint & 4 & 7 \\
air volume & 5 & 5 \\ % [1ex] adds vertical space
other temp. & 6 & 5 \\ % [1ex] adds vertical space
\hline %inserts single line
\end{tabular}
\label{table:spec} % is used to refer this table in the text
\end{table}

\subsection{Baseline and Metrics}
As a baseline to compare our proposed approach against, we adpot a simple feature extraction scheme for each trace $F=\{med, var\}$, where $med$ and $var$ is just the $median$ and $variance$ computed over the entire trace.

For classification, we measure the averaged cross-validation accuracy in two different scenarios (intra- and inter- buildings). For identifying potential 
misclassifications, we choose the true-positive rate (TPR, also known as recall rate) and false-positive rate (FPR) as metrics to evaluate the performance of 
our entropy-based aprroach. In our case, a true-positive (TP) is when an instance considered to be misclassified is actually a correct classification while a 
false-positive (FP) is when an instance considered to be misclassfied is misclassified.

\subsection{Intra Building Performance}

\subsection{Inter Building Performance}

\subsection{Window Length Sensitivity and Training Bootstrapping}

\subsection{Feature Importance and Selection}

\subsection{Identifying Misclassification}

\subsection{Case Study}
